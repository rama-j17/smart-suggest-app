# -*- coding: utf-8 -*-
"""model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rPCLB4eOQpt4XzWt4Ef6c-D7ghfWA6Jm
"""

import torch
from torch import nn

# Define your model class
class GRUAttentionRecModel(nn.Module):
    def __init__(self, num_items, num_categories, emb_dim=64, cat_emb_dim=16, hidden_dim=128):
        super(GRUAttentionRecModel, self).__init__()
        self.item_emb = nn.Embedding(num_items, emb_dim)
        self.cat_emb = nn.Embedding(num_categories, cat_emb_dim)
        self.gru = nn.GRU(emb_dim + cat_emb_dim, hidden_dim, batch_first=True)
        self.attn = nn.Linear(hidden_dim, 1)
        self.fc = nn.Linear(hidden_dim, num_items)

    def forward(self, items, cats, return_attn=False):
        item_vecs = self.item_emb(items)
        cat_vecs = self.cat_emb(cats)
        x = torch.cat([item_vecs, cat_vecs], dim=2)
        gru_out, _ = self.gru(x)
        attn_scores = self.attn(gru_out).squeeze(-1)
        attn_weights = torch.softmax(attn_scores, dim=1)
        context = torch.sum(gru_out * attn_weights.unsqueeze(-1), dim=1)
        out = self.fc(context)
        if return_attn:
            return out, attn_weights
        return out