# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zqvi_W2wqxzVMSo6raJ8jPYtBUyruFHH
"""

import streamlit as st
import torch
import torch.nn as nn
import gdown
from huggingface_hub import hf_hub_download
import os
from model import GRUAttentionRecModel
from utils import load_encoders

# Configuration for the model
MODEL_REPO_NAME = 'rama-j17/smart-suggest-app'  # Replace with your Hugging Face repo name
MODEL_FILE_NAME = 'best_smartsuggest_model.pt'  # Model file name on Hugging Face

# Function to download the model using huggingface_hub
def download_model():
    model_path = hf_hub_download(repo_id=MODEL_REPO_NAME, filename=MODEL_FILE_NAME)
    return model_path

# Load model from Hugging Face Hub
def load_model():
    model_path = download_model()
    model = GRUAttentionRecModel(num_items=10000, num_categories=100)  # Adjust the number of items and categories
    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
    model.eval()
    return model

# Load encoders (item and category)
def load_encoders():
    item_encoder = joblib.load("models/item_encoder.pkl")
    category_encoder = joblib.load("models/category_encoder.pkl")
    return item_encoder, category_encoder

# Initialize model and encoders
model = load_model()
item_encoder, category_encoder = load_encoders()

# Streamlit UI for product suggestions
st.title('Smart Suggestion System')

# Input: Session History
session_history = st.text_area("Enter session history (comma-separated item IDs):")

if session_history:
    session_items = list(map(int, session_history.split(',')))

    # Process the input session history
    session_input = torch.tensor(session_items).unsqueeze(0)  # Add batch dimension
    category_input = torch.zeros_like(session_input)  # Placeholder, modify as needed

    # Get predictions from the model
    with torch.no_grad():
        logits = model(session_input, category_input)
        top_k = torch.topk(logits, 5, dim=1).indices.squeeze().tolist()

    # Decode the top K predictions into item names
    top_items = item_encoder.inverse_transform(top_k)

    st.write(f"Top 5 recommended items: {top_items}")
