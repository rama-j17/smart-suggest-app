# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zqvi_W2wqxzVMSo6raJ8jPYtBUyruFHH
"""

import streamlit as st
import torch
import torch.nn as nn
import gdown
from huggingface_hub import hf_hub_download
import joblib
import os
from model import GRUAttentionRecModel
from utils import load_encoders

# App Config
st.set_page_config(page_title="SmartSuggest", layout="centered")

# Title and Description
st.title("üß† SmartSuggest: Intelligent Product Recommender")
st.write(
    "Enter your recent browsing session as a list of item IDs. "
    "Our GRU + Attention model will recommend products you're likely to engage with next."
)

# Configuration for the model
MODEL_REPO_NAME = 'rama-j17/smart-suggest-app'  # Replace with your Hugging Face repo name
MODEL_FILE_NAME = 'best_smartsuggest_model.pt'  # Model file name on Hugging Face

# Function to download the model using huggingface_hub
def download_model():
    model_path = hf_hub_download(repo_id=MODEL_REPO_NAME, filename=MODEL_FILE_NAME)
    return model_path

# Load model from Hugging Face Hub
def load_model():
    model_path = download_model()
    model = GRUAttentionRecModel(num_items=234838, num_categories=496)  # Adjust the number of items and categories
    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
    model.eval()
    return model

# Load encoders (item and category)
def load_encoders():
    item_encoder = joblib.load("models/item_encoder.pkl")
    category_encoder = joblib.load("models/category_encoder.pkl")
    return item_encoder, category_encoder

# Initialize model and encoders
model = load_model()
item_encoder, category_encoder = load_encoders()

# Input: Session History
st.subheader("üì• Input Session History")
session_history = st.text_area(
    "Enter comma-separated item IDs from your session:",
    placeholder="e.g. 130, 76060, 17114"
)

# Handle input and run model
if session_history:
    try:
        session_items = list(map(int, session_history.strip().split(',')))
        if not session_items:
            st.warning("Please enter at least one item ID.")
        else:
            st.success(f"Session received: {session_items}")

            # Prepare model input
            session_input = torch.tensor(session_items).unsqueeze(0)  # Add batch dimension
            category_input = torch.zeros_like(session_input)  # Placeholder, or real categories if available

            # Get top 5 recommendations
            with torch.no_grad():
                logits = model(session_input, category_input)
                top_k = torch.topk(logits, 5, dim=1).indices.squeeze().tolist()

            # Decode item IDs
            top_items = item_encoder.inverse_transform(top_k)

            st.subheader("üéØ Top 5 Recommended Items")
            st.markdown("These are personalized recommendations based on your session:")
            for rank, item in enumerate(top_items, 1):
                st.markdown(f"**{rank}.** `{item}`")

    except Exception as e:
        st.error(f"‚ö†Ô∏è Error processing input: {e}")
else:
    st.info("Please enter your session history above to see recommendations.")
